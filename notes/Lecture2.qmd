---
title: "Lecture 2"
execute:
  freeze: auto  # re-render only when source changes
format: 
    revealjs:
        theme: solarized
        incremental: true
        preview-links: auto
        chalkboard: true
engine: julia
julia:
  exeflags: ["--project=../PopGen25"]
---

# Lecture 2: Describing genetic diversity

## Genetic diversity writ large

Last week you learned how different evolutionary forces affect allele frequencies at a single site.

. . .

This week, we'll be discussing how we can measure genetic diversity and what those measurements can tell us.

. . .

We will look at a few statistics that measure genetic diversity, notably the SFS, $\pi$, $\theta_W$ and $H$.

. . .

Before that, we'll talk about why we even care.

## What is diversity good for?

*Evolution is the change in traits of populations over generations*

. . .

Change is impossible if variation does not exist.

. . .

Genetic variation allows for evolvability - without it populations are stuck.

. . .

*Most* variation might in fact be deleterious in any particular environment.

. . .

Conversely, if the environment changes - old variants are unlikely to meet all of its needs.

## Where does diversity come from?

Ultimately, all genetic diversity begins with mutation.

. . .

A key quantity in population genetics is $\theta$ - the neutral genetic diversity.

. . .

Under Wright-Fisher, $\theta = 4 N_e \mu$, where $N_e$ is the *effective* population size, and $\mu$ is the *neutral* mutation rate.

. . .

So, diversity should depend on both the population size and mutation rates. Does it?

## Measuring diversity at a site

Before we think about what's happening across the genome, we can start by asking what is happening at an individual locus.

::: callout-tip
## Recall

We will refer to the allele frequency of the *alternate* allele at site $j$ as $p$.
:::

What are some ways we can ask how diverse a population is at a site?

## Expected Heterozygosity ( *H* )

One of the simplest metrics is to ask how frequently we see individuals who are heterozygous.

. . .

Let's start by generalizing to any number of alleles.

. . .

$$
H_j = 1 - \sum_{i=1}^{k}{p_i^2}
$$

Where $p_i$ is the frequency of the $i^{th}$ allele and there are a total of $k$ alleles.

. . .

If there are only two alleles:

$$1-p_a^2 -p_A^2 = 2p_a(1-p_a)$$

. . .

In practice - often not the case: why?

## Sample heterozygosity (*h*) {.smaller}

Another way to think about genetic diversity is to ask:

What is the probability that if you sample any two alleles in the population, they are different.

. . .

In an infinite population with a biallelic site, this is simply $2p(2-p)$

. . .

But populations are not infinite, and we also don't sample all possible individuals.

. . .

Instead, we calculate the expected *sample* heterozygosity, and account for number of alleles sampled ($n$):

$$
h_j=\frac{n}{n-1}\left(1-\sum_i^k{p_i^2}\right)
$$

## Heterozygosity and drift

We saw last week how drift alone leads to evolutionary change by fixation and loss of alleles due to random chance.

. . .

This can lead to a change in heterozygosity that is quite predictable.

. . .

Any generation, the given (true) heterozygosity is $H_t$. What will it be in the next generation?

. . .

The traditional proof for this starts by first defining $\mathscr{H}$ : the Homozygosity of the population.

. . .

Useful to know: $H = 1-\mathscr{H}$.

## How does drift change homozygosity?

If two randomly chosen alleles had the same ancestor last generation: they are homozygous.

Alternatively, if they both come from different alleles, but homozygous ones.

So: $\mathscr{H}_{t+1} = \frac{1}{2N} + (1-\frac{1}{2N})\mathscr{H_t}$

Then:

$$
H_{t+1} = 1-\mathscr{H}_{t+1}=\frac{1}{2N}+\left(1-\frac{1}{2N}\right)\left(1-H_t\right)=\left(1-\frac{1}{2N}\right)H_t
$$

In other words - drift causes heterozygosity to predictably decrease every generation (at a rate of $-\frac{H}{2N}$)

## Heterozygosity across a region (π)

We can then zoom out and ask what the diversity is across a whole region. Say there are $L$ sites in your region, then the total *polymorphism* is given by:

$$
\pi = \sum_{j=1}^{L}{h_j}
$$

. . .

The more common definition you'll see is the *per site* *polymorphism/heterozygosity*, given as:

$$
\pi =  \frac{1}{L}\sum_{j=1}^{L}{h_j}
$$

Note, often the two will be labeled as $\pi$, but per-site is more common.

## Let's check some intuition

Here, we'll use the example data from Hahn's textbook:

```{julia}
using Plots
theme(:solarized)
default(legendfontsize=18,guidefontsize=18)
fake_data = 
["T" "T" "A" "C" "A" "A" "T" "C" "C" "G" "A" "T" "C" "G" "T";
 "-" "-" "A" "C" "G" "A" "T" "G" "C" "G" "C" "T" "C" "G" "T";
 "T" "C" "A" "C" "A" "A" "T" "G" "C" "G" "A" "T" "G" "G" "A";
 "T" "T" "A" "C" "G" "A" "T" "G" "-" "-" "C" "T" "C" "G" "T"]
[join(i) for i in eachrow(fake_data)]
```

Each row is an individual, each column is a site. Notice how some sites have missing data "-".

. . .

What is the heterozygosity at the third site?

. . .

How can we account for missing data in calculating heterozygosity?

## Code examples

```{julia}
#| echo: true
using StatsBase

function sample_h(alleles)
    obs_alleles=collect(skipmissing(alleles)) #Genotypes might be missing - we skip those.
    n=length(obs_alleles)
    freqs = values(countmap(obs_alleles)) ./ n #Get allele frequencies for each allele
    return( n/(n-1)*(1-sum(freqs .^ 2))) #Return the expected sample h
end

function sample_π(genotype_matrix)
    return(sum([sample_h(genotype_matrix[:,x]) for x in 1:size(genotype_matrix)[2]])/size(genotype_matrix)[2]) #Sum of h over all sites/number of sites
end

sample_π(replace(fake_data,"-"=>missing))
```

## What should $\pi$ look like?

We can use some baseline Wright-Fisher model with mutation assumptions to figure out what $\pi$ will be like in practice.

. . .

Let's say there are $N$ individuals. The mutation rate is $\mu$. What is the expected heterozygosity?

## Finding steady-states

One way to solve modeling problems is to enumerate all of the forces acting on them, and find when the forces cancel each other out.

. . .

Heterozygosity can increase because of mutation.

. . .

Heterozygosity is lost because of drift (recall drift eventually causes alleles to be fixed/lost.

. . .

How strong are these forces?

## More formally {.smaller}

Let $H_t$ be the heterozygosity at time $t$.

Then:

$$
H_{t+1}-H_t = \Delta mutation + \Delta drift
$$

Any individuals *not* heterozygous can become so via mutation at either of their two copies of the locus.

$$
\Delta mutation = (1-H)2\mu
$$

And drift cause the loss of heterozygosity at the rate we derived earlier:

$\Delta drift = -\frac{H}{2N}$

Then, at steady state:

$$
0=(1-H_t)2\mu-\frac{H_t}{2N}
$$

Chalboard time!

## Expected heterozygosity

The neutral expected heterozygosity is $4N\mu$. This is often called $\theta$.

. . .

Of course, the quantity is different for haploids: $\theta = 2N\mu$

. . .

What does this tell us about average allele frequencies in populations?

. . .

Remember: $H = 2p(1-p) = 4N\mu$

## Not the only heterozygosity

$\pi$ is a commonly used estimator for genetic diversity, but it has some issues. The biggest is that when true diversity is high, variance in *sample* diversity is also really high. That is, it's hard to get an accurate estimate.

```{julia}
using Plots, LaTeXStrings, Measures
function var_pi(θ,n)
    (n+1)/(3*(n-1))*θ+2(n^2+n+3)/(9*n*(n-1))*θ^2
end
ns=range(10,1000,length=50)
thetas = [var_pi.(theta,ns) for theta in [0.01,0.1,0.5,1.0]]
plot(ns,thetas,xlabel="n",ylabel=L"Var(\pi)",label=["0.01" "0.1" "0.5" "1.0"],margins=5mm)
```

## Alternatives to $\pi$ : $\theta_W$

One common alternative is to use Watterson's Theta. The idea is that we can simply look at the *number* of segregating sites $S$ (sites with at least one variant). Because as we sample more individuals, we are more likely to sample a new allele, there is a correction factor:

$$
\theta_W = \frac{S}{\sum_{i=1}^{n-1}{1/i}}
$$

```{julia}
#| echo: true
function wat_theta(gm)
    S = sum([is_segregating(i) for i in eachcol(gm)])
    a = sum([1/i for i in 1:size(gm)[1]])
    return(S/a)
end
```

## $\theta_W$ - lower variance with greater sampling

Unlike $\pi$, $\theta_W$ shows a nice decrease in sample variance with more samples.

```{julia}
function theta_var(n,theta)
    vars =sum([theta/i for i in 1:(n-1)])+sum([theta^2/(i^2) for i in 1:(n-1)])
    a = sum([1/i for i in 1:(n-1)])
    return(vars/(a^2))
end

thetas_w = [theta_var.(ns,i) for i in [0.01,0.1,0.5,1.0]]
plot(ns,thetas_w,xlabel="n",ylabel=L"Var(\theta_W)",label=["0.1" "1" "5" "10"],margins=5mm)
```

## Why is $\theta_W$ not the standard?

Accuracy from sample size is not the only potential issue:

. . .

$\theta_W$ is not robust to other sources of errors:

-   missing data

-   sequencing error

-   uneven sampling

. . .

In practice, it's often good to be able to look at multiple different estimators, and make informed conclusions about the data from them.

## Single data point for whole genome?

$\theta$, $\pi$, $h$ and similar are *summary statistics* - they reduce potentially huge amounts of data into a single number. That *can* be useful, but sometimes it's better to look at a denser view of the data. Let's make a larger/more complicated data-set.

```{julia}
include("../src/useful_code.jl")
ancestral=fake_alignment(1,10000)
geno_matrix = fake_alignment_biallelic(ancestral,30;gaps=0.01)
[join(replace(i,missing=>"-")) for i in eachrow(geno_matrix)]
```

## Windowed statistics {.smaller}

```{julia}
#| echo: true
function window_map(f,data,wnd_size)
    start = 1
    stop = start+wnd_size
    centers = []
    result = []
    while(stop < size(data)[2])
        append!(centers,(start+stop)/2)
        append!(result,f(data[:,start:stop])/wnd_size)
        start = stop
        stop = start + wnd_size
    end
    return(centers,result)
end

plot(window_map(sample_π,geno_matrix,1000),label="Windowed",xlabel="Window Center",ylabel=L"\pi",size=(1000,500))
```

## But window choice not arbitrary

As we'll discuss in the next lecture: our choice of windows might depend on factors like recombination rate.

. . .

Frequently, studies use a *single* window value. But different parts of the genome have different degrees of LD, mutation, recombination.

. . .

In general - good idea to run a sliding window analysis.

## Sliding windows

```{julia}
function sliding_window_map(f,data,wnd_size,wnd_step)
    start = 1
    stop = start+wnd_size
    centers = []
    result = []
    while(stop < size(data)[2])
        append!(centers,(start+stop)/2)
        append!(result,f(data[:,start:stop])/wnd_size)
        start = start + wnd_step
        stop = start + wnd_size
    end
    return(centers,result)
end

plot!(sliding_window_map(sample_π,geno_matrix,1000,250),label="Sliding",xlabel="Window Center",ylabel=L"\pi")
```

## Longer alignment - more information

One good way to represent a whole region is to summarize the allele frequencies at each site. This is called the *Site Frequency Spectrum*, or (*Allele Frequency Spectrum*). In our complicated alignment, it helps us see the frequency of different alleles.

## SFS example:

```{julia}
#| echo: false
function SFS(gm) 
    return([minimum(values(countmap(i)))/length(i) for i in eachcol(gm)]); 
end

histogram(SFS(only_segregating(geno_matrix)),legend=false,xlabel="Minor Allele Frequency",ylabel="Count",margins=5mm,bins=30,size=(1000,500))
```

## Allele *polarity*: ancestral state

When you don't know what the ancestral allele is, you generally examine the *minor* allele frequency (whichever is rarer.

. . .

This results in a *folded* SFS.

. . .

But, if you have an outgroup/ancestral data, you can instead use the full SFS.

## What do you do with the SFS

One of the uses of the SFS is, ironically, to calculate summary statistics.

. . .

Some, like $\theta_H$ estimate the genetic diversity.

. . .

Others, like Tajima's D (which we'll talk about more later) let you estimate the influence of demography on the population.

## 

## How is variation distributed among individuals?

We can now say something about how genetic variation is sumarised across a population. *But* this gives us very little information about how the variation is distributed within each individual.

For instance, if overall $\pi$ is high - does that mean that every individual is likely to *be* heterozygous?

## Summarizing complex data: PCA {.smaller}

Principal component analysis (PCA) is an approach to take complex, highly dimensional data (like the genotypes of a bunch of individuals at very many sites), and reduce it to something simpler.

```{julia}
using MultivariateStats
x = randn(1000)
y = x+randn(1000)

pca_test_plot=scatter(x,y,leg=false,size=(900,500))

```

## PC1 identifies the axis of highest variability

```{julia}
pca_m = fit(PCA,transpose(hcat(x,y)))
plot!(pca_test_plot,i->i*scaled_loadings(pca_m)[1,1],-4,4)
```

## Each PC explains the next most variation {.smaller}

When data fits PCA assumptions, each subsequent PC explains the next most variance.

. . .

Technical details: the first principal component is the eigenvector of the correspondingly largest eigenvalue of a covariance matrix between data points.

. . .

i.e.: You correlate all data points, and ask what linear transformations explain the covariation. This in turn explains the most variation in the data.

. . .

## Works in genomic data too!

Here's our toy sequence data along the first two PCs.

```{julia}
gm_PCA = fit(PCA,transpose(geno_mat_to_Int(geno_matrix)))
scatter(gm_PCA.proj[:,1],gm_PCA.proj[:,2],leg=false,size=(1000,500))
```

## Is PCA always worth it? {.smaller}

How do you know how many PCs are "enough" to describe your data?

. . .

PCA significance can be evaluated using the *broken stick model*.

. . .

If you choose *n* random break points of a stick of length 1, then the size of these segments follows a typical distribution:

```{julia}
scatter(reverse([broken_stick_dist(i,10) for i in 1:10]),legend=false,xlabel="PC",ylabel="Variance explained",margins=5mm,size=(800,400),xticks=(1:10))
```

## How does that compare to our toy sequence data?

```{julia}
p1 = scree(pca_m;broken_stick=true)
p2 = scree(gm_PCA;broken_stick=true)
plot(p1,p2,size=(1000,600))
```